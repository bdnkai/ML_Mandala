{
 "cells": [
  {
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "id": "a3fd713f-2bd4-47f8-9933-2b022a03048d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "noteable": {
     "cell_type": "code"
    },
    "is_executing": true
   },
   "id": "b886bf04-1ca2-4a1f-9cc8-60de3584807e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, discount, get_legal_actions):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self,state,action,value):\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "        return max(self.get_qvalue(state, action) for action in possible_actions)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "        qvalue = (1-learning_rate)*self.get_qvalue(state, action) + learning_rate*(reward + gamma*self.get_value(next_state))\n",
    "        self.set_qvalue(state, action, qvalue)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "        return max(possible_actions, key = lambda action: self.get_qvalue(state, action))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        action = None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            action = self.get_best_action(state)\n",
    "        return action"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "noteable": {
     "cell_type": "code"
    },
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:09.031002+00:00",
     "start_time": "2023-05-28T10:30:08.868879+00:00"
    }
   },
   "id": "a0b303ab-2b53-484e-b408-ac4fd4eb0658"
  },
  {
   "cell_type": "code",
   "source": [
    "class DragonOrbEnvironment:\n",
    "    def __init__(self):\n",
    "        # Initialize the state and the total reward\n",
    "        self.state = None\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state and the total reward\n",
    "        self.state = None\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update the state and the total reward based on the action\n",
    "        # This is a placeholder and should be replaced with your own logic\n",
    "        self.state = None\n",
    "        self.total_reward += 0\n",
    "\n",
    "        # Check if the game is over\n",
    "        done = False\n",
    "\n",
    "        return self.state, self.total_reward, done\n",
    "\n",
    "    def render(self):\n",
    "        # Display the current state of the environment\n",
    "        # This is a placeholder and should be replaced with your own logic\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "noteable": {
     "cell_type": "code"
    },
    "ExecuteTime": {
     "end_time": "2023-05-28T10:30:55.711956+00:00",
     "start_time": "2023-05-28T10:30:55.553380+00:00"
    }
   },
   "id": "8a70ba40-913b-4e19-9e3e-6d2acd2c8a1b"
  }
 ],
 "metadata": {
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "4da391bd-5dd5-5fae-8474-4a53daaa4908",
    "openai_ephemeral_user_id": "259c52a0-d341-58b8-b742-0c6abff5529e",
    "openai_subdivision1_iso_code": "US-CA"
   }
  },
  "noteable": {
   "last_transaction_id": "8797b6fe-65dc-4165-a3f7-7be40549af7d",
   "last_delta_id": "2c6223c9-cb3c-4176-a93f-2f60ac8bff06"
  },
  "selected_hardware_size": "small",
  "nteract": {
   "version": "noteable@2.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
